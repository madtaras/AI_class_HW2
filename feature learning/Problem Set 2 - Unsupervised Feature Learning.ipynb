{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Unsupervised feature learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem you will see how unsupervised learning can help you train better models even with labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_x = 32 # width of image\n",
    "image_y = 32 # height of image\n",
    "patch_dim = 8 # height/width of a patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image(object):\n",
    "\n",
    "    def __init__(self,data,label,patches):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        -----------\n",
    "        Takes image related data, called on image creation.\n",
    "        \"\"\"\n",
    "        self.label = label # image label\n",
    "        self.patches = patches.transpose().tolist()\n",
    "        \n",
    "        self.__img_data = data\n",
    "\n",
    "    def view(self):\n",
    "        \"\"\"\n",
    "        Function: View\n",
    "        --------------\n",
    "        Call function to view RGB image\n",
    "        \"\"\"\n",
    "        from PIL import Image\n",
    "        im = Image.fromarray(self.__img_data)\n",
    "        im = im.resize((128,128),Image.BILINEAR)\n",
    "        im.show()\n",
    "\n",
    "    def get_label(self):\n",
    "        \"\"\"\n",
    "        Function: Label\n",
    "        ---------------\n",
    "        Returns label of image\n",
    "        \"\"\"\n",
    "        return self.label\n",
    "\n",
    "    def get_patches(self):\n",
    "        \"\"\"\n",
    "        Function: Patches\n",
    "        -----------------\n",
    "        Returns list of patch vectors. Each patch length patch_size\n",
    "        \"\"\"\n",
    "        return self.patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_helper(name,m):\n",
    "    channels = 3\n",
    "    patch_dim = 8\n",
    "    patches_per_image = (image_x/patch_dim)*(image_y/patch_dim)\n",
    "\n",
    "    images = np.fromfile('data/images_'+name+'.bin',dtype=np.uint8)\n",
    "    images = images.reshape((m,image_x,image_y,channels))\n",
    "\n",
    "    patches = np.fromfile('data/patches_'+name+'.bin',dtype=np.float32)\n",
    "    patches = patches.reshape((patch_dim**2,-1))\n",
    "\n",
    "    labels = np.fromfile('data/labels_'+name+'.bin',dtype=np.uint8)\n",
    "\n",
    "    image_list = []\n",
    "    for i in range(images.shape[0]):\n",
    "        image_list.append(Image(images[i,...],labels[i],\n",
    "          patches[:,int(i*patches_per_image):int((i+1)*patches_per_image)]))\n",
    "    \n",
    "    return image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_helper(patches,num):\n",
    "    from PIL import Image\n",
    "    \n",
    "    xnum = int(np.sqrt(num))\n",
    "    if xnum**2 == num:\n",
    "        ynum = xnum\n",
    "    else:\n",
    "        ynum = xnum+1\n",
    "\n",
    "    imDim = 50\n",
    "\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        patches = patches-np.min(patches)\n",
    "        patches = patches/np.max(patches)\n",
    "        patchDim = patches.shape[0]\n",
    "        image = np.zeros(((patchDim+1)*ynum+1,(patchDim+1)*xnum+1))\n",
    "        for i in range(ynum):\n",
    "            for j in range(xnum):\n",
    "                imnum = i*xnum+j\n",
    "                if imnum>=num:\n",
    "                    break\n",
    "                ax = plt.subplot2grid((ynum,xnum),(i,j))\n",
    "                ax.imshow(patches[:,:,i*xnum+j].squeeze(), cmap = plt.get_cmap('gray'))\n",
    "                ax.axes.get_xaxis().set_visible(False)\n",
    "                ax.axes.get_yaxis().set_visible(False)\n",
    "                \n",
    "        plt.subplots_adjust(wspace=-.5 ,hspace=0.2)\n",
    "        plt.show()\n",
    "        return\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # rescale to be [0-255]\n",
    "    patches = patches-np.min(patches)\n",
    "    patches = 255*patches/np.max(patches)\n",
    "\n",
    "    newpatches = np.empty((imDim,imDim,num))\n",
    "\n",
    "    for p in range(num):\n",
    "        patch = patches[:,:,p].squeeze().copy()\n",
    "        im = Image.fromarray(patch)\n",
    "        im = im.resize((imDim,imDim),Image.BILINEAR)\n",
    "        newpatches[:,:,p] = np.asarray(im.convert('L'))\n",
    "\n",
    "    patches = newpatches\n",
    "    image = np.zeros(((imDim+1)*ynum+1,(imDim+1)*xnum+1))\n",
    "\n",
    "    for i in range(ynum):\n",
    "        for j in range(xnum):\n",
    "            imnum = i*xnum+j\n",
    "            if imnum>=num:\n",
    "                break\n",
    "            image[i*(imDim+1)+1:i*(imDim+1)+imDim+1, \\\n",
    "                  j*(imDim+1)+1:j*(imDim+1)+imDim+1] \\\n",
    "                  = patches[:,:,imnum]\n",
    "    image = Image.fromarray(image, 'L')\n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_features(images):\n",
    "    \"\"\"\n",
    "    Extracts raw pixel features for all images.  Returns a 2-D array\n",
    "    of size featDim x numExamples and a vector of labels.\n",
    "    \"\"\"\n",
    "    X = [np.array(image.get_patches()).ravel() for image in images]\n",
    "    X = np.vstack(X).transpose() # featdim by num samples\n",
    "    # label array\n",
    "    Y = np.array([image.get_label() for image in images])\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_patches(patches):\n",
    "    \"\"\"\n",
    "    Function: View Patches\n",
    "    ----------------------\n",
    "    Pass in an array of patches (or centroids) in order to view them as\n",
    "    images.\n",
    "    \"\"\"\n",
    "    view_helper(patches.reshape(patch_dim,patch_dim,-1),patches.shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_images = 2000\n",
    "file_tag = 'train'\n",
    "train_image_list = load_helper(file_tag,num_train_images)\n",
    "\n",
    "num_test_images = 1000\n",
    "file_tag = 'test'\n",
    "test_image_list = load_helper(file_tag,num_test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,train_Y = pixel_features(train_image_list)\n",
    "test_X,test_Y = pixel_features(test_image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1024, 2000), (2000,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.77245879e+00 -2.36186966e-01  4.86198068e-02 ... -2.42185146e-01\n",
      "  -1.57253239e-02  9.44859445e-01]\n",
      " [-2.66454482e+00 -1.67018801e-01  1.09784670e-01 ... -1.25524580e-01\n",
      "  -4.30927604e-01  1.60894477e+00]\n",
      " [ 1.04330039e+00 -5.38556993e-01  1.20907120e-01 ... -3.12024087e-01\n",
      "   2.23523572e-01  3.41507196e-01]\n",
      " ...\n",
      " [ 1.45803440e+00  3.45114350e-01  2.76380658e-01 ... -7.04077601e-01\n",
      "   9.04016048e-02  1.22188699e+00]\n",
      " [ 2.40676475e+00 -1.25334633e-03 -8.77847150e-02 ... -2.81709522e-01\n",
      "  -4.84565198e-01  1.68098342e+00]\n",
      " [ 2.08898202e-01 -3.34747881e-02  5.40620685e-02 ... -1.38029903e-01\n",
      "   3.84905010e-01  1.27014351e+00]]\n",
      "\n",
      "[1 1 1 1 0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(train_X[:10])\n",
    "print(\"\")\n",
    "print(train_Y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train logistic regression on the raw pixel data and report the train and test set results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.894\n",
      "Test score 0.493\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='newton-cg').fit(train_X.reshape(2000,1024), train_Y)\n",
    "\n",
    "print('Train score', clf.score(train_X.reshape(2000,1024), train_Y))\n",
    "print('Test score', clf.score(test_X.reshape(1000,1024), test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train SVM on the raw pixel data and report the train and test set results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.8995\n",
      "Test score 0.496\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "LinearSVC_clf = svm.LinearSVC()\n",
    "LinearSVC_clf.fit(train_X.reshape(2000,1024), train_Y)  \n",
    "\n",
    "print('Train score', LinearSVC_clf.score(train_X.reshape(2000,1024), train_Y))\n",
    "print('Test score', LinearSVC_clf.score(test_X.reshape(1000,1024), test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train XGBoost on the raw pixel data and report the train and test set results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.9845\n",
      "Test score 0.468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(train_X.reshape(2000,1024), train_Y)\n",
    "\n",
    "print('Train score', model.score(train_X.reshape(2000,1024), train_Y))\n",
    "print('Test score', model.score(test_X.reshape(1000,1024), test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning better features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of hand-designing better features let us see if we can learn them directly from data. Each image is a 32x32 grid of pixels. We will divide the image into sixteen 8x8 \"patches\". Next, we will use K-means to cluster all the patches into centroids. These centroids will then allow us to use a better feature representation of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how we can get patches from the images and visualize them. Make sure you understand the dimensions of every array and what they mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 2\n",
    "patches = np.hstack([np.array(image.get_patches()).transpose() for image in train_image_list[:num_images]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.77245879,  1.8376137 ,  0.10936241, ..., -2.39925647,\n",
       "        -0.78283018, -0.79868215],\n",
       "       [-2.66454482, -0.29113939,  0.08760805, ..., -1.94315505,\n",
       "        -0.33032835, -0.81574214],\n",
       "       [ 1.04330039, -3.0411675 , -0.48515356, ...,  0.63409501,\n",
       "         0.48942992, -0.7069785 ],\n",
       "       ...,\n",
       "       [-0.71922368, -1.05667174, -2.60033393, ..., -0.24897462,\n",
       "        -1.07012224, -0.04645052],\n",
       "       [-0.62359619,  0.39362317, -0.08528376, ..., -0.08314735,\n",
       "        -0.47935438, -0.15645838],\n",
       "       [-0.56007594,  0.93923163,  0.39494473, ...,  0.21429826,\n",
       "        -0.47797999, -0.24591312]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run k-means from scikit-learn to group all patches into clusters. Initially, pick the number of clusters according to your best guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 6 1 2 1 5 1 1 4 5 6 5 2 2 8 1 5 5 6 6 1 8 8 8 4 4 4 1 1 1 8 1 2 4 7 5 2\n",
      " 2 2 1 2 7 7 3 3 3 1 2 1 1 7 9 9 9 2 5 2 5 1 2 9 0 0 3]\n",
      "[[-0.67140993 -0.33152428 -1.34280884 ... -0.21186523 -0.06614874\n",
      "  -0.25142996]\n",
      " [ 0.2276333   0.59216746 -0.06446162 ...  0.45980409 -0.07172625\n",
      "   0.03976033]\n",
      " [-0.34820413 -0.74487471  0.24650291 ... -0.17323199 -0.10067776\n",
      "  -0.02982391]\n",
      " ...\n",
      " [ 0.3810439  -1.37698575 -0.16424916 ...  0.05485701 -0.17993792\n",
      "   0.73250041]\n",
      " [-0.81732165 -0.49676028 -0.164701   ... -0.48896798 -0.06061512\n",
      "   0.06899509]\n",
      " [-1.18297681 -0.72538073 -0.12475144 ... -0.05432108 -0.12247644\n",
      "   0.43134966]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_images = 1000\n",
    "patches = np.hstack([np.array(image.get_patches()).transpose() for image in train_image_list[:num_images]])\n",
    "patches.shape\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(patches)\n",
    "print(kmeans.labels_)\n",
    "print(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, visualize the centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "centroids = np.array(kmeans.cluster_centers_) # Please use this variable name for the array of centroids\n",
    "\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=200, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing examples in a new way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you have the centroids defining similar groups in your patches. Represent every image in your training and test set in distances between the patch and each centroid. For example, if you used 10 clusters and each image has 16 patches, new representation of the image will be a vector of 160 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train all three classifiers from the above (logistic regression, SVM and XGBoost) on the new image representation. Report the train and test set results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the best out of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In industry, we typically try to get as much as possible out of the data we have. Try different number of clusters and different configuration of the models and report the best accuracy you got on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
